---
title: "Problem Set 3 Solutions"
author: "Megan Hoeksema"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```

## Introduction

Please complete the following tasks regarding the data in R. Please generate a solution document in R markdown and upload the .Rmd document and a rendered  .doc, .docx, or .pdf document.  Your work should be based  on the data's being in the same folder as the .Rmd file. Please turn in your work on Canvas. Your solution document should have your answers to the questions and should display the requested plots.

These questions were rendered in R markdown through RStudio (<https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>, <http://rmarkdown.rstudio.com> ).

## Question 1

Suppose $(S,M,P)_{\boldsymbol \theta}$ is a parametrized family of distributions. The parameter $\boldsymbol \theta$ may be vector-valued or one dimensional. Under fairly general circumstances, the maximum likelihood parameter estimate $\hat{\boldsymbol \theta}$ of the parameter $\boldsymbol \theta$ based on a sample $\{X_1,X_2,...X_n\}$ is *consistent*. Informally, this means that as larger and larger samples are used to estimate the parameter, the estimate gets closer and closer to the true value. 

Some parameter estimates are *unbiased*. Informally, this means that if the estimate is applied to $M$ samples of size $n$ to get a collection of estimates $\left\{\hat{\boldsymbol \theta}_1,\hat{\boldsymbol \theta}_2,...\hat{\boldsymbol \theta}_M\right\}$, the mean of the estimates, $\frac{1}{M}\sum_{i=1}^M\hat{\boldsymbol \theta}_i$ will get closer and closer to $\hat{\boldsymbol \theta}$ as $M$ gets larger and larger.

In this question you will perform numerical experiments on samples from a $Normal(\mu,\sigma^2)$ to see whether the maximum likelihood estimates for $\mu$ and $\sigma^2$ appear to be consistent and unbiased. 

### Question 1.a

(5 points)

The purpose of this question is to perform numerical experiments to gain insight into the whether of maximum likelihood estimates of $\mu$ and $\sigma^2$ are consistent for samples from $Normal(0,1)$. 

The code provided generates $N=10,000$ samples $\{x_1,x_2,...x_N\}$ from the standard Normal distribution, $Normal(0,1)$. For each value $n$ in $\{1,2,3,...N\}$, the maximum likelihood estimates of $\mu$ and $\sigma^2$ are computed for the initial portion $\{x_1,x_2,...x_n\}$ of the sample $\{x_1,x_2,...x_N\}$. These values are stored in order of $n$ in the data frame "dat.consist" with the variable names "mu.hat" and "sigma.sq.hat" respectively. A column of the corresponding values of $n$ is added under the variable name "n". Please use this data frame to examine whether these samples provide numerical evidence that the maximum likelihood estimates of $\mu$ and $\sigma^2$ are consistent. Plotting using "geom_line" may be helpful.

```{r}
set.seed(12345)
N<-10000
samp<-rnorm(N)
theta.est<-function(n,s=samp){
  m<-mean(s[1:n])
  s2<-sum((s[1:n]-m)^2)/n
  return(c(m,s2))
}
dat.consist<-t(sapply(1:N,theta.est))
dat.consist<-data.frame(dat.consist)
dat.consist$n<-1:N
names(dat.consist)<-c("mu.hat","sigma.sq.hat","n")

ggplot(dat.consist,aes(x=n,y=mu.hat))+geom_line()
dat.consist$mu.hat[c(10,100,500,1000,5000,10000)]

ggplot(dat.consist,aes(x=n,y=sigma.sq.hat))+geom_line()
dat.consist$sigma.sq.hat[c(10,100,500,1000,5000,10000)]

### Conclusion: As n grows larger, the estimates for mu.hat get closer and closer to 0. As n grows larger, the estimates for sigma.sq.hat become closer and closer to 1.
```


### Question 1.b

(5 points)

The purpose of this question is to perform numerical experiments to gain insight into whether the maximum likelihood estimates of $\mu$ and $\sigma^2$ are unbiased for samples of size 5 from $Normal(0,1)$ 

Create 10,000 samples of size 5 from the standard Normal distribution and calculate the maximum likelihood estimates $\hat{\mu}$ and $\hat{\sigma}^2$  of $\mu$ and $\sigma^2$ for each sample. Compute the mean of the $\hat{\mu}$s and the mean of the $\hat{\sigma}^2$s. Does the maximum likelihood estimate of $\mu$ seem to be unbiased? (You may repeat the experiment to help answer this question.) Does the maximum likelihood estimate of $\mu$ seem to be unbiased? Does the maximum likelihood estimate of $\sigma^2$ seem to be unbiased? (Try comparing with the adjusted estimates produced by dividing the sum of the squared differences by 4 instead of 5.)

```{r}
set.seed(45678)
mat<-matrix(rnorm(10000*5),ncol=5)

mu.hat<-apply(mat,1,mean)
  sigma2.ml<-function(x){sum((x-mean(x))^2)/5}
sigma2.hat<-apply(mat,1,sigma2.ml)
sigma2.b<-apply(mat,1,var)
qplot(mu.hat, bins =60)
mean(mu.hat)
mean(sigma2.hat)
qplot(sigma2.hat)
mean(sigma2.b)

## Divide by 4 instead of 5
mu.hat2<-apply(mat,1,mean)
  sigma2.2.ml<-function(x){sum((x-mean(x))^2)/4}
sigma2.2.hat<-apply(mat,1,sigma2.2.ml)
sigma2.2.b<-apply(mat,1,var)
qplot(mu.hat2, bins =60)
mean(sigma2.2.hat)
qplot(sigma2.2.hat)
mean(sigma2.2.b)

##Conclusion: The maximum likelihood estimate of mu seems unnbiased. It remains close to 0, however the maximum likelihood estimate of sigma2 seems to be biased as dividing by 4 instead of 5 gives a better estimate of the variance.

```


## Question 2

(10 points)

The exponential distributions are a one parameter family of continuous distributions, $Exp(\lambda)$. Given $\lambda$, the sample space is $[0,\infty)$ and the probability density function is $f(x)=\lambda\exp(-\lambda x)$. (The exponential distributions are used to model waiting times to events such as arrival of jobs in a queue.)

If $x_1...x_n$ are $n$ independent draws from an exponential distribution with parameter $\lambda$, the likelihood function of this sample is $\prod_{i=1}^{n}\lambda\exp(-\lambda x_{i})$. Please derive the maximum likelihood value of $\lambda$ as a function of $x_1...x_n$. That is, given $x_1...x_n$, what value of $\lambda$ maximizes $\prod_{i=1}^{n}\lambda\exp(-\lambda x_{i})$?

The joint likelihood of the independent samples is $$\prod _{i=1}^n\lambda\exp(-\lambda x_i)$$

$$ln f(\lambda) = ln [\prod _{i=1}^n\lambda\exp(-\lambda x_i)$$ 
               $$ = \sum _{i=1}^n ln[\lambda\exp(-\lambda x_i)]$$
               $$ = \sum _{i=1}^n [ln(\lambda)  + ln\exp(-\lambda x_i)]$$
               $$ = \sum _{i=1}^n ln(\lambda) - \sum _{i=1}^n \lambda x_i$$
               $$ = nln(\lambda) - \lambda\sum _{i=1}^n x_i$$
               $$ \dfrac {dg}{d\lambda} = n * \dfrac {1}{\lambda}  - \sum _{i=1}^n \ x_i = 0 $$
              $$ \dfrac {1}{\lambda} = \dfrac {1}{n}\sum _{i=1}^n \ x_i = \overline{x} $$
              $$ \lambda = \dfrac {n}{\sum _{i=1}^n \ x_i} = \dfrac {1}{\overline{x}}  $$
          
## Question 3

This question deals with the one parameter family of Poisson distributions, $Poisson(\lambda)$. For the parameter value $\lambda$, the sample space is $\mathbb{N}=\{0,1,2,...n,...\}$ and the density function $f$ is defined by $f(k)=\frac{\lambda^ke^{-\lambda}}{k!}$ for $k\in\mathbb{N}$. 

The Poisson distributions are among those used to model data that are counts of occurrences, such as the number of arrivals in a queue during a fixed time period. One can show that the Poisson distributions and the exponential distributions are related. If a phenomenon has the property that there exists a value $\gamma$ such that the number of occurrences in any time interval of length $t$ has a Poisson distribution with parameter $\lambda=\gamma t$, then the distribution of the time to the next occurrence in an exponential distribution with parameter $\gamma$. 

For very frequent occurrences, the counts may be approximated by Normal distributions to allow use of a wider range of statistical tools. The following questions examine the accuracy of this approximation.  

### 3.a.

(5 points)

One step in approximating a Poisson distribution by a Normal distribution is to identify appropriate values of $\mu$ and $\sigma^2$. 

For $\lambda\in\{4,25,100\}$, create samples of size 100,000 from the Poisson distribution using "rpois" with parameter $\lambda$. For each sample, please find the maximum likelihood estimate of $\mu$ and $\sigma^2$ if the samples are viewed as samples from a $Normal(\mu,\sigma^2)$ distribution. 


```{r}
lambdas<-c(4,25,100)
set.seed(345678)
mat.sim<-matrix(rep(NA,100000*3),ncol=3)
for(i in 1:3){
  mat.sim[,i]<-rpois(100000,lambdas[i])
}

apply(mat.sim,2,mean)
apply(mat.sim,2,var)

var(1:3)

var_pop <- function(x) {
  mean((x - mean(x))^2)
}

var_pop(1:3)
```

### 3.b.

(5 points)

For the values of $\lambda$ and for integers $x$ with the lower bound of 0 and the upper bounds of qpois(.99995,4), qpois(.99995,25), and qpois(.99995,100), respectively, provide visualizations comparing the probability of an outcome equal to $x$ under $Poisson(\lambda)$ and the probability of the event $(x-.5,x+.5)$ under the Normal distribution with $\mu=\lambda$ and $\sigma^2=\lambda$. 

The use of the probability of a value in $(x-.5,x+.5)$ under the Normal distribution adjusts for the fact that $Normal(\mu=\lambda,\sigma^2=\lambda)$ is a continuous distribution. Values in $(x-.5,x+.5)$ round to $x$. For example, the probability of $k=85$ under $Poisson(100)$, which equals `r dpois(85,100)`, is approximated by the probability of the event $(84.5,85.5)$ under $Normal(\mu=100,\sigma^2=100)$, `r pnorm(85.5,100,10)-pnorm(84.5,100,10)`.

Also, for each $\lambda$, give the sum of the absolute differences of the two probabilities, for all the values of $x$ assessed. A template is provided for this.

Does the proposed Normal approximation to $Poisson(\lambda)$ improve as $\lambda$ increases?

```{r}
lambdas<-c(4,25,100)
lim.max<-c(qpois(.99995,4), qpois(.99995,25), qpois(.99995,100))
for(i in 1:length(lambdas)){
  x<-0:lim.max[i]
  # Vectorization allows the values to be computed for all x's 
  # at once.
  lower<-pnorm(x-.5,mean=lambdas[i],sd=sqrt(lambdas[i]))
  upper<-pnorm(x+.5,mean=lambdas[i],sd=sqrt(lambdas[i]))
  p.pois<-dpois(x,lambda=lambdas[i])
  p.norm<-upper-lower
  dat.temp<-data.frame(x=0:lim.max[i],
                d=dpois(0:lim.max[i],lambda=lambdas[i]),
                d.approx=p.norm)
  g<-ggplot(data=dat.temp,aes(x=x))+geom_col(aes(y=d),color="gray",fill="gray")+
    geom_point(aes(y=d.approx))
  print(g)
}

lim.max<-c(qpois(.99995,4), qpois(.99995,25), qpois(.99995,100))
for(i in 1:length(lambdas)){
  x<-0:lim.max[i]
  lower.prob<-pnorm(x-.5,mean=lambdas[i],sd=sqrt(lambdas[i]))
  upper.prob<-pnorm(x+.5,mean=lambdas[i],sd=sqrt(lambdas[i]))
  p.pois<-dpois(x,lambda=lambdas[i])
  p.norm<-upper.prob-lower.prob
  approx.error<-sum(abs(p.pois-p.norm))
  print(approx.error)
}

### Conclusion: The normal approximation improves as lamda increases.
```



## Question 4

Please supply the missing code where indicated.

The data sets in these questions were downloaded 1/17/2020 from https://ourworldindata.org/ 

The code chunks below read in a data frame of world populations and a data frame of world population densities. 

```{r}
dat.pop<-read.csv("population.csv",stringsAsFactors = FALSE)
dat.den<-
  read.csv("population-density.csv",stringsAsFactors = FALSE)
names(dat.den)[4]<-"density"
```

### 4.a.

(2 points)

Write code to restrict both data frames to cases in which the value of "Year" is 2000 and the value of "Code" is not the empty string, "", or the value for the whole world,"OWID_WRL". 

```{r} 
dat.pop<-filter(dat.pop,Year==2000,!Code %in% c("","OWID_WRL"))
dat.den<-filter(dat.den,Year==2000,!Code %in% c("","OWID_WRL"))
```

Merge the data sets.

```{r}
dat.both<-inner_join(dat.den,dat.pop,by="Code")
# check: this will equal 1 if the code above is correct.
mean(dat.both$Entity.x==dat.both$Entity.y)
```

### 4.b.

(3 points)

Write code to find the indices in "dat.both" at which the population takes on its minimum or maximum value and at which the density takes on its minimum or maximum value. Store the resulting indices in a vector named "inds". Use of the "which" function can simplify this effort.

```{r}
inds<-c(
  which(dat.both$density %in%
              c(max(dat.both$density),min(dat.both$density))),
  which(dat.both$Population %in%
            c(max(dat.both$Population),min(dat.both$Population)))
)

```

### 4.c.

(3 points)

Use "transmute" from dplyr to create a data frame from dat.both with the value for "entity",the log of "density" in "den.log", and the log of "Population" in "pop.log".

```{r}
dat.both<-transmute(dat.both,den.log=log(density),
               pop.log=log(Population),entity=Entity.x)
```


Create a data frame "dat.text" from dat.both that includes only the rows containing these extremes. 

```{r}
dat.text<-dat.both[inds,]
```

### 4.d.

(2 points)

Use "ggplot" to create a point plot of the log of population (on the x-axis) versus the log of density. Store the plot in the variable g. Display the plot. 

```{r}
g<-ggplot(dat.both,aes(x=pop.log,y=den.log))+geom_point()
g
```

The following when uncommented should give the previous plot with the names of the entities having extreme population or extreme density, assuming that the result of the "transmute" call was stored back in "dat.both".

```{r}

 g<-g+
   geom_text(data=dat.text,aes(x=pop.log,y=den.log,label=entity))
 g
```

### 4.e.

(10 points)

Please add the least squares best fit line with "pop.log" as the $x$-value and "den.log" as the $y$-value in $\mathbf{y}=m\mathbf{x}+b$. Also plot the line minimizing the squared error $\sum\left(x_i-(ly_i+c))^2\right)$ again with "pop.log" as the $x$-value and "den.log" as the $y$-value in such a way that the points $(x,y)$ on the line are related by $x=ly+c$. That is, if $f$ is the function giving "pop.log" as an affine function of "den.log", minimizing the square error  $\sum\left(x_i-(ly_i+c))^2\right)$, plot the inverse function $f^{-1}$. 

```{r}

lin.model<-lm(den.log~pop.log,data=dat.both)

coeffs<-lm(den.log~pop.log,data=dat.both)$coefficients
coeffs

coeffs.rev<-lm(pop.log~den.log,data=dat.both)$coefficients
slope.rev<-1/coeffs.rev[2]
intercept.rev<- (-coeffs.rev[1])/coeffs.rev[2]

g<-g+
  geom_abline(slope=coeffs[2],intercept=coeffs[1],color="blue")
g<-g+
  geom_abline(slope=slope.rev,intercept=intercept.rev,
              color="orange")
g
```



